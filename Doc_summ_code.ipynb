{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e4a49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import fitz  # pymupdf\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('punkt_tab', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    print(\"Warning: Could not download NLTK data\")\n",
    "\n",
    "# ML and Deep Learning imports\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Try to import evaluation metrics\n",
    "try:\n",
    "    import evaluate\n",
    "    rouge_available = True\n",
    "except ImportError:\n",
    "    rouge_available = False\n",
    "    print(\"Warning: evaluate library not available, ROUGE scores will be skipped\")\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# 1. PDF utilities\n",
    "def pdf_to_text(path):\n",
    "    try:\n",
    "        doc = fitz.open(path)\n",
    "        pages = [page.get_text(\"text\") for page in doc if page.get_text(\"text\").strip()]\n",
    "        doc.close()\n",
    "        full_text = \"\\n\\n\".join(pages)\n",
    "        full_text = re.sub(r\"[^\\S\\r\\n]+\", \" \", full_text)\n",
    "        full_text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", full_text)\n",
    "        return full_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF {path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def chunk_text(text, max_tokens=400, overlap=50):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        end_idx = min(i + max_tokens, len(words))\n",
    "        chunk = \" \".join(words[i:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        if end_idx >= len(words):\n",
    "            break\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "# 2. CSV Diagnostic Tool\n",
    "def diagnose_csv_structure(file_path):\n",
    "    print(\"=\"*80)\n",
    "    print(\"CSV FILE DIAGNOSTIC TOOL\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Basic file inspection\n",
    "    print(\"\\n1. BASIC FILE INSPECTION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_lines = [f.readline().strip() for _ in range(5)]\n",
    "\n",
    "        print(\"First 5 lines of the file:\")\n",
    "        for i, line in enumerate(first_lines):\n",
    "            print(f\"Line {i+1}: {line[:200]}...\" if len(line) > 200 else f\"Line {i+1}: {line}\")\n",
    "\n",
    "        separators = [',', ';', '\\t', '|']\n",
    "        separator_counts = {sep: first_lines[0].count(sep) for sep in separators}\n",
    "        print(f\"\\nPotential separators found: {separator_counts}\")\n",
    "        likely_separator = max(separator_counts, key=separator_counts.get)\n",
    "        print(f\"Most likely separator: '{likely_separator}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Try different loading methods\n",
    "    print(\"\\n2. TRYING DIFFERENT LOADING METHODS\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    loading_results = {}\n",
    "\n",
    "    for method, kwargs in [\n",
    "        ('standard', {}),\n",
    "        ('quote_minimal', {'quoting': csv.QUOTE_MINIMAL}),\n",
    "        ('quote_none', {'quoting': csv.QUOTE_NONE, 'on_bad_lines': 'skip', 'engine': 'python'})\n",
    "    ]:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, **kwargs)\n",
    "            non_null_score = df.iloc[:, :2].notna().sum().sum() if df.shape[1] >= 2 else 0\n",
    "            loading_results[method] = {\n",
    "                'success': True,\n",
    "                'shape': df.shape,\n",
    "                'columns': df.columns.tolist()[:5],\n",
    "                'score': non_null_score\n",
    "            }\n",
    "        except Exception as e:\n",
    "            loading_results[method] = {'success': False, 'error': str(e)}\n",
    "\n",
    "    for method, result in loading_results.items():\n",
    "        print(f\"\\n{method.upper()}:\")\n",
    "        if result['success']:\n",
    "            print(f\"  Shape: {result['shape']}\")\n",
    "            print(f\"  Columns: {result['columns']}\")\n",
    "            print(f\"  Score: {result['score']}\")\n",
    "        else:\n",
    "            print(f\"  Failed: {result['error']}\")\n",
    "\n",
    "    # Select best method\n",
    "    best_method = max(\n",
    "        (method for method in loading_results if loading_results[method]['success']),\n",
    "        key=lambda m: loading_results[m]['score'],\n",
    "        default=None\n",
    "    )\n",
    "\n",
    "    if best_method:\n",
    "        kwargs = {\n",
    "            'standard': {},\n",
    "            'quote_minimal': {'quoting': csv.QUOTE_MINIMAL},\n",
    "            'quote_none': {'quoting': csv.QUOTE_NONE, 'on_bad_lines': 'skip', 'engine': 'python'}\n",
    "        }[best_method]\n",
    "        df = pd.read_csv(file_path, **kwargs)\n",
    "        return df, best_method\n",
    "    return None, None\n",
    "\n",
    "def analyze_columns_for_summarization(df):\n",
    "    print(\"\\n3. COLUMN ANALYSIS FOR SUMMARIZATION\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        print(\"No data to analyze\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "    # Analyze columns\n",
    "    column_stats = {}\n",
    "    for col in df.columns:\n",
    "        non_null_count = df[col].count()\n",
    "        if non_null_count > 0:\n",
    "            sample_texts = df[col].dropna().head(1000).astype(str)\n",
    "            column_stats[col] = {\n",
    "                'non_null_count': non_null_count,\n",
    "                'avg_char_length': sample_texts.str.len().mean(),\n",
    "                'avg_word_count': sample_texts.str.split().str.len().mean(),\n",
    "                'sample': str(sample_texts.iloc[0])[:150] + \"...\" if len(str(sample_texts.iloc[0])) > 150 else str(sample_texts.iloc[0])\n",
    "            }\n",
    "\n",
    "    print(\"\\nColumn Statistics:\")\n",
    "    for col, stats in column_stats.items():\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Non-null: {stats['non_null_count']}\")\n",
    "        print(f\"  Avg length: {stats['avg_char_length']:.1f} chars, {stats['avg_word_count']:.1f} words\")\n",
    "        print(f\"  Sample: {stats['sample']}\")\n",
    "\n",
    "    # Find best document-summary pairs\n",
    "    best_pairs = []\n",
    "    columns = list(column_stats.keys())\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(len(columns)):\n",
    "            if i != j:\n",
    "                col1, col2 = columns[i], columns[j]\n",
    "                valid_pairs = (~df[col1].isnull() & ~df[col2].isnull()).sum()\n",
    "\n",
    "                if valid_pairs >= 1000:  # Require more pairs for large dataset\n",
    "                    stats1, stats2 = column_stats[col1], column_stats[col2]\n",
    "                    doc_col, sum_col = (col1, col2) if stats1['avg_char_length'] > stats2['avg_char_length'] else (col2, col1)\n",
    "                    doc_len, sum_len = column_stats[doc_col]['avg_char_length'], column_stats[sum_col]['avg_char_length']\n",
    "                    length_ratio = sum_len / (doc_len + 1)\n",
    "                    quality_score = valid_pairs * (1 / (abs(length_ratio - 0.15) + 0.1))  # Target ~0.15 compression\n",
    "\n",
    "                    best_pairs.append({\n",
    "                        'document_col': doc_col,\n",
    "                        'summary_col': sum_col,\n",
    "                        'valid_pairs': valid_pairs,\n",
    "                        'doc_avg_len': doc_len,\n",
    "                        'sum_avg_len': sum_len,\n",
    "                        'compression_ratio': length_ratio,\n",
    "                        'quality_score': quality_score\n",
    "                    })\n",
    "\n",
    "    if best_pairs:\n",
    "        return max(best_pairs, key=lambda x: x['quality_score'])\n",
    "    return None\n",
    "\n",
    "def suggest_corrected_loading(file_path):\n",
    "    df, method = diagnose_csv_structure(file_path)\n",
    "\n",
    "    if df is None:\n",
    "        print(\"\\nCould not load CSV file. Please check file path, format, and encoding.\")\n",
    "        return None\n",
    "\n",
    "    best_mapping = analyze_columns_for_summarization(df)\n",
    "\n",
    "    if best_mapping is None:\n",
    "        print(\"\\nNo suitable document-summary pairs found.\")\n",
    "        return None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RECOMMENDED CONFIGURATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Document column: '{best_mapping['document_col']}'\")\n",
    "    print(f\"Summary column: '{best_mapping['summary_col']}'\")\n",
    "    print(f\"Valid pairs: {best_mapping['valid_pairs']}\")\n",
    "    print(f\"Avg document length: {best_mapping['doc_avg_len']:.1f} chars\")\n",
    "    print(f\"Avg summary length: {best_mapping['sum_avg_len']:.1f} chars\")\n",
    "    print(f\"Compression ratio: {best_mapping['compression_ratio']:.3f}\")\n",
    "\n",
    "    return {'loading_method': method, **best_mapping}\n",
    "\n",
    "# 3. Data Loading\n",
    "def load_and_clean_data_corrected(file_path, force_columns=None, chunksize=10000):\n",
    "    print(f\"Loading data from: {file_path}\")\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"CSV file not found: {file_path}\")\n",
    "\n",
    "    chunks = []\n",
    "    if force_columns:\n",
    "        method = force_columns.get('loading_method', 'standard')\n",
    "        doc_col = force_columns['document_col']\n",
    "        sum_col = force_columns['summary_col']\n",
    "\n",
    "        print(f\"Using columns: {doc_col} -> document, {sum_col} -> summary\")\n",
    "        kwargs = {\n",
    "            'standard': {},\n",
    "            'quote_minimal': {'quoting': csv.QUOTE_MINIMAL},\n",
    "            'quote_none': {'quoting': csv.QUOTE_NONE, 'on_bad_lines': 'skip', 'engine': 'python'}\n",
    "        }[method]\n",
    "\n",
    "        for chunk in pd.read_csv(file_path, chunksize=chunksize, **kwargs):\n",
    "            chunk = chunk[[doc_col, sum_col]].copy()\n",
    "            chunk.columns = ['document', 'summary']\n",
    "            chunk = chunk.dropna(subset=['document', 'summary'])\n",
    "            chunk['document'] = chunk['document'].astype(str).str.strip()\n",
    "            chunk['summary'] = chunk['summary'].astype(str).str.strip()\n",
    "            chunk = chunk[(chunk['document'] != '') & (chunk['summary'] != '')]\n",
    "            chunks.append(chunk)\n",
    "    else:\n",
    "        print(\"No column mapping provided. Running diagnostic...\")\n",
    "        best_mapping = suggest_corrected_loading(file_path)\n",
    "        if best_mapping is None:\n",
    "            print(\"Falling back to original method...\")\n",
    "            return load_and_clean_data_original(file_path)\n",
    "        return load_and_clean_data_corrected(file_path, best_mapping, chunksize)\n",
    "\n",
    "    df = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame(columns=['document', 'summary'])\n",
    "    print(f\"Final cleaned dataset: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def load_and_clean_data_original(file_path, chunksize=10000):\n",
    "    print(f\"Using original loading method for: {file_path}\")\n",
    "\n",
    "    for encoding in ['utf-8', 'latin-1', 'cp1252', 'utf-16']:\n",
    "        try:\n",
    "            chunks = []\n",
    "            for chunk in pd.read_csv(\n",
    "                file_path,\n",
    "                encoding=encoding,\n",
    "                engine=\"python\",\n",
    "                quoting=csv.QUOTE_NONE,\n",
    "                on_bad_lines=\"skip\",\n",
    "                sep=',',\n",
    "                chunksize=chunksize\n",
    "            ):\n",
    "                if len(chunk.columns) >= 2:\n",
    "                    chunk = chunk.iloc[:, :2].copy()\n",
    "                    chunk.columns = ['document', 'summary']\n",
    "                    chunk = chunk.dropna(subset=['document', 'summary'])\n",
    "                    chunk['document'] = chunk['document'].astype(str).str.strip()\n",
    "                    chunk['summary'] = chunk['summary'].astype(str).str.strip()\n",
    "                    chunk = chunk[(chunk['document'] != '') & (chunk['summary'] != '')]\n",
    "                    chunks.append(chunk)\n",
    "            df = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame(columns=['document', 'summary'])\n",
    "            print(f\"Successfully loaded with encoding: {encoding}\")\n",
    "            print(f\"Final cleaned dataset: {df.shape}\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed with encoding {encoding}: {e}\")\n",
    "    raise ValueError(\"Could not read CSV with any encoding.\")\n",
    "\n",
    "# 4. EDA\n",
    "def perform_comprehensive_eda(df, save_plots=True, sample_size=5000):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EFFICIENT EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    if save_plots:\n",
    "        os.makedirs(\"eda_plots\", exist_ok=True)\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    sample_df = df.sample(min(sample_size, len(df)), random_state=42) if len(df) > sample_size else df\n",
    "    print(f\"Using sample of {len(sample_df)} rows for detailed analysis\")\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\n1. DATASET OVERVIEW\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(\"\\nFirst few samples:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Document (first 150 chars): {str(df.iloc[i]['document'])[:150]}...\")\n",
    "        print(f\"Summary (first 100 chars): {str(df.iloc[i]['summary'])[:100]}...\")\n",
    "\n",
    "    # Missing values\n",
    "    print(\"\\n2. MISSING VALUES ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Text length analysis\n",
    "    print(\"\\n3. TEXT LENGTH ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    sample_df['doc_length'] = sample_df['document'].str.len()\n",
    "    sample_df['doc_word_count'] = sample_df['document'].str.split().str.len()\n",
    "    sample_df['sum_length'] = sample_df['summary'].str.len()\n",
    "    sample_df['sum_word_count'] = sample_df['summary'].str.split().str.len()\n",
    "    sample_df['compression_ratio'] = sample_df['sum_length'] / sample_df['doc_length']\n",
    "\n",
    "    length_stats = pd.DataFrame({\n",
    "        'Document_chars': sample_df['doc_length'].describe(),\n",
    "        'Summary_chars': sample_df['sum_length'].describe(),\n",
    "        'Document_words': sample_df['doc_word_count'].describe(),\n",
    "        'Summary_words': sample_df['sum_word_count'].describe(),\n",
    "        'Compression_ratio': sample_df['compression_ratio'].describe()\n",
    "    })\n",
    "\n",
    "    print(\"Length Statistics:\")\n",
    "    print(length_stats.round(2))\n",
    "\n",
    "    # Plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Text Length Distributions', fontsize=16)\n",
    "\n",
    "    axes[0, 0].hist(sample_df['doc_length'], bins=50, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Document Character Length')\n",
    "    axes[0, 0].set_xlabel('Characters')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "    axes[0, 1].hist(sample_df['sum_length'], bins=50, alpha=0.7, color='lightcoral')\n",
    "    axes[0, 1].set_title('Summary Character Length')\n",
    "    axes[0, 1].set_xlabel('Characters')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "    axes[0, 2].hist(sample_df['compression_ratio'], bins=50, alpha=0.7, color='lightgreen')\n",
    "    axes[0, 2].set_title('Compression Ratio')\n",
    "    axes[0, 2].set_xlabel('Summary Length / Document Length')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1, 0].hist(sample_df['doc_word_count'], bins=50, alpha=0.7, color='orange')\n",
    "    axes[1, 0].set_title('Document Word Count')\n",
    "    axes[1, 0].set_xlabel('Words')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1, 1].hist(sample_df['sum_word_count'], bins=50, alpha=0.7, color='purple')\n",
    "    axes[1, 1].set_title('Summary Word Count')\n",
    "    axes[1, 1].set_xlabel('Words')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "    axes[1, 2].scatter(sample_df['doc_length'], sample_df['sum_length'], alpha=0.5)\n",
    "    axes[1, 2].set_title('Document vs Summary Length')\n",
    "    axes[1, 2].set_xlabel('Document Length')\n",
    "    axes[1, 2].set_ylabel('Summary Length')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_plots:\n",
    "        plt.savefig('eda_plots/length_distributions.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Vocabulary analysis\n",
    "    print(\"\\n4. VOCABULARY ANALYSIS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    stop_words = set(stopwords.words('english')) if 'stopwords' in nltk.data.find('corpora') else set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to'])\n",
    "\n",
    "    doc_words = []\n",
    "    sum_words = []\n",
    "    for text in sample_df['document']:\n",
    "        words = word_tokenize(str(text).lower())\n",
    "        doc_words.extend([word for word in words if word.isalpha() and word not in stop_words])\n",
    "    for text in sample_df['summary']:\n",
    "        words = word_tokenize(str(text).lower())\n",
    "        sum_words.extend([word for word in words if word.isalpha() and word not in stop_words])\n",
    "\n",
    "    doc_vocab = Counter(doc_words)\n",
    "    sum_vocab = Counter(sum_words)\n",
    "\n",
    "    print(f\"Document vocabulary size: {len(doc_vocab)}\")\n",
    "    print(f\"Summary vocabulary size: {len(sum_vocab)}\")\n",
    "    print(f\"Vocabulary overlap: {len(set(doc_vocab.keys()) & set(sum_vocab.keys()))}\")\n",
    "    print(\"\\nMost common words in documents:\")\n",
    "    for word, count in doc_vocab.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "    print(\"\\nMost common words in summaries:\")\n",
    "    for word, count in sum_vocab.most_common(10):\n",
    "        print(f\"  {word}: {count}\")\n",
    "\n",
    "    if doc_words and sum_words:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        doc_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(doc_words))\n",
    "        axes[0].imshow(doc_wordcloud, interpolation='bilinear')\n",
    "        axes[0].set_title('Document Word Cloud')\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        sum_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(sum_words))\n",
    "        axes[1].imshow(sum_wordcloud, interpolation='bilinear')\n",
    "        axes[1].set_title('Summary Word Cloud')\n",
    "        axes[1].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        if save_plots:\n",
    "            plt.savefig('eda_plots/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    return sample_df\n",
    "\n",
    "# 5. Preprocessing\n",
    "def advanced_preprocessing(df):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ADVANCED PREPROCESSING\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    initial_count = len(df)\n",
    "    print(f\"Initial dataset size: {initial_count}\")\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = str(text)\n",
    "        text = text.replace('story_separator_special_tag', '').replace('replace_table_token_7_th', '')\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    df['document'] = df['document'].apply(clean_text)\n",
    "    df['summary'] = df['summary'].apply(clean_text)\n",
    "\n",
    "    # Calculate lengths for filtering\n",
    "    df['doc_word_count'] = df['document'].str.split().str.len()\n",
    "    df['sum_word_count'] = df['summary'].str.split().str.len()\n",
    "    df['doc_length'] = df['document'].str.len()\n",
    "    df['sum_length'] = df['summary'].str.len()\n",
    "    df['compression_ratio'] = df['sum_length'] / df['doc_length']\n",
    "\n",
    "    # Relaxed filters based on EDA\n",
    "    len_filter = (\n",
    "        (df['doc_word_count'] >= 1000) &  # Min 1000 words (from EDA: min ~1807)\n",
    "        (df['doc_word_count'] <= 2500) &  # Max 2500 words (from EDA: max ~2064)\n",
    "        (df['sum_word_count'] >= 20) &    # Min 20 words (from EDA: min ~24)\n",
    "        (df['sum_word_count'] <= 600) &   # Max 600 words (from EDA: max ~484)\n",
    "        (df['doc_length'] >= 5000) &      # Min 5000 chars (from EDA: min ~9357)\n",
    "        (df['sum_length'] >= 150)         # Min 150 chars (from EDA: min ~178)\n",
    "    )\n",
    "\n",
    "    df_filtered = df[len_filter].copy()\n",
    "    print(f\"After length filtering: {len(df_filtered)} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "    # Compression ratio filter\n",
    "    compression_filter = (\n",
    "        (df_filtered['compression_ratio'] >= 0.05) &  # Min 5% (from EDA: min ~0.02)\n",
    "        (df_filtered['compression_ratio'] <= 0.30)    # Max 30% (from EDA: max ~0.25)\n",
    "    )\n",
    "\n",
    "    df_filtered = df_filtered[compression_filter].copy()\n",
    "    print(f\"After compression ratio filtering: {len(df_filtered)} ({len(df_filtered)/initial_count*100:.1f}%)\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    initial_filtered = len(df_filtered)\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=['document'], keep='first').reset_index(drop=True)\n",
    "    print(f\"After duplicate removal: {len(df_filtered)} (removed {initial_filtered - len(df_filtered)} duplicates)\")\n",
    "\n",
    "    # Fallback with more relaxed filters if needed\n",
    "    if len(df_filtered) < 1000:\n",
    "        print(\"WARNING: Insufficient data after filtering. Using relaxed filters...\")\n",
    "        relaxed_filter = (\n",
    "            (df['doc_word_count'] >= 500) &\n",
    "            (df['sum_word_count'] >= 10) &\n",
    "            (df['doc_length'] >= 3000) &\n",
    "            (df['sum_length'] >= 100) &\n",
    "            (df['compression_ratio'] >= 0.01) &\n",
    "            (df['compression_ratio'] <= 0.50)\n",
    "        )\n",
    "        df_filtered = df[relaxed_filter].copy()\n",
    "        print(f\"Using relaxed filters: {len(df_filtered)} samples\")\n",
    "\n",
    "    # Clean up temporary columns\n",
    "    df_filtered = df_filtered[['document', 'summary']].reset_index(drop=True)\n",
    "    print(f\"Final preprocessed dataset size: {len(df_filtered)}\")\n",
    "\n",
    "    if len(df_filtered) < 1000:\n",
    "        raise ValueError(f\"Insufficient data after preprocessing: {len(df_filtered)} samples\")\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "# 6. Data Splitting\n",
    "def create_balanced_splits(df, train_size=0.7, val_size=0.3, random_state=42):\n",
    "    print(f\"\\nCreating balanced splits: {train_size:.1%} train, {val_size:.1%} validation\")\n",
    "\n",
    "    if len(df) < 1000:\n",
    "        raise ValueError(f\"Need at least 1000 samples, got {len(df)}\")\n",
    "\n",
    "    # Stratify by document length\n",
    "    df['length_bin'] = pd.qcut(df['document'].str.len(), q=5, duplicates='drop')\n",
    "    train_df, val_df = train_test_split(\n",
    "        df,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df['length_bin']\n",
    "    )\n",
    "\n",
    "    train_df = train_df[['document', 'summary']].reset_index(drop=True)\n",
    "    val_df = val_df[['document', 'summary']].reset_index(drop=True)\n",
    "\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "\n",
    "    print(\"\\nSplit Statistics:\")\n",
    "    print(f\"Train - Avg doc length: {train_df['document'].str.len().mean():.1f}\")\n",
    "    print(f\"Train - Avg sum length: {train_df['summary'].str.len().mean():.1f}\")\n",
    "    print(f\"Val - Avg doc length: {val_df['document'].str.len().mean():.1f}\")\n",
    "    print(f\"Val - Avg sum length: {val_df['summary'].str.len().mean():.1f}\")\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "# 7. Tokenization\n",
    "def preprocess_data_enhanced(train_df, val_df, tokenizer, max_input_length=512, max_target_length=128):\n",
    "    print(f\"\\nTokenizing data (max_input: {max_input_length}, max_target: {max_target_length})\")\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        inputs = [\"summarize: \" + doc for doc in examples['document']]\n",
    "        targets = examples['summary']\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            inputs,\n",
    "            max_length=max_input_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "\n",
    "        labels = tokenizer(\n",
    "            text_target=targets,\n",
    "            max_length=max_target_length,\n",
    "            truncation=True,\n",
    "            padding=False\n",
    "        )\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "    train_tokenized = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names\n",
    "    )\n",
    "\n",
    "    val_tokenized = val_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=val_dataset.column_names\n",
    "    )\n",
    "\n",
    "    print(f\"Tokenized train size: {len(train_tokenized)}\")\n",
    "    print(f\"Tokenized validation size: {len(val_tokenized)}\")\n",
    "\n",
    "    return train_tokenized, val_tokenized\n",
    "\n",
    "# 8. Trainer Setup\n",
    "def setup_simple_trainer(train_dataset, val_dataset, model_name=\"google/mt5-small\"):\n",
    "    print(f\"Setting up model: {model_name}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=-100\n",
    "    )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        if not rouge_available:\n",
    "            return {}\n",
    "\n",
    "        predictions, labels = eval_pred\n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "        try:\n",
    "            rouge = evaluate.load(\"rouge\")\n",
    "            result = rouge.compute(\n",
    "                predictions=decoded_preds,\n",
    "                references=decoded_labels,\n",
    "                use_stemmer=True\n",
    "            )\n",
    "            return {k: round(v, 4) for k, v in result.items()}\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing ROUGE: {e}\")\n",
    "            return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./enhanced-mt5-financial\",\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=3e-5,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=3,\n",
    "        warmup_steps=100,\n",
    "        eval_strategy=\"no\",  # Disable evaluation during training to avoid memory issues\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=400,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=False,  # Disable since no evaluation\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\",\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics if rouge_available else None,  # Only if available\n",
    "    )\n",
    "\n",
    "    return trainer, tokenizer, model\n",
    "\n",
    "# 9. Inference\n",
    "def generate_summary_enhanced(text, model, tokenizer, max_length=128, device=\"cpu\"):\n",
    "    input_text = f\"summarize: {text}\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=True\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summary_ids = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            min_length=20,\n",
    "            num_beams=4,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3,\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# 10. Testing\n",
    "def test_with_your_data(model, tokenizer, df, n_samples=5, device=\"cpu\"):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TESTING MODEL ON YOUR DATA\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    samples = df.sample(n_samples, random_state=42)\n",
    "    for i, row in samples.iterrows():\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Input (first 100 chars): {row['document'][:100]}...\")\n",
    "        print(f\"Reference (first 100 chars): {row['summary'][:100]}...\")\n",
    "        print(f\"Generated: {generate_summary_enhanced(row['document'], model, tokenizer, device=device)}\")\n",
    "\n",
    "def quick_test_enhanced(model, tokenizer, device=\"cpu\"):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"QUICK MODEL TEST\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    test_examples = [\n",
    "        \"\"\"The company reported strong quarterly results with revenue of $2.8 billion, representing a 15% increase year-over-year.\n",
    "        Net income reached $450 million, up from $380 million in the same quarter last year. The growth was primarily driven by\n",
    "        increased demand in the cloud services division, which saw a 28% revenue increase.\"\"\",\n",
    "        \"\"\"The Federal Reserve announced its decision to maintain the current federal funds rate at 5.25-5.50%, marking the\n",
    "        fourth consecutive meeting without a rate change. The central bank cited recent economic data showing moderate inflation\n",
    "        trends and stable employment levels as key factors in the decision.\"\"\"\n",
    "    ]\n",
    "\n",
    "    for i, example in enumerate(test_examples):\n",
    "        print(f\"\\nTest Example {i+1}:\")\n",
    "        print(f\"Input: {example[:100]}...\")\n",
    "        print(f\"Generated: {generate_summary_enhanced(example, model, tokenizer, device=device)}\")\n",
    "\n",
    "# 11. Main Pipeline\n",
    "def main_pipeline_improved(file_path=\"merged.csv\", save_plots=True):\n",
    "    print(\"=\"*80)\n",
    "    print(\"IMPROVED FINANCIAL SUMMARIZATION PIPELINE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    try:\n",
    "        # Step 1: Load data\n",
    "        print(\"\\nStep 1: Loading data efficiently...\")\n",
    "        best_mapping = suggest_corrected_loading(file_path)\n",
    "        if best_mapping is None:\n",
    "            raise ValueError(\"Failed to identify suitable document-summary columns\")\n",
    "\n",
    "        df = load_and_clean_data_corrected(file_path, best_mapping)\n",
    "        if df.shape[0] < 1000:\n",
    "            raise ValueError(f\"Insufficient data loaded: {df.shape[0]} samples\")\n",
    "\n",
    "        # Step 2: EDA\n",
    "        print(\"\\nStep 2: Performing efficient EDA...\")\n",
    "        df_with_stats = perform_comprehensive_eda(df, save_plots)\n",
    "\n",
    "        # Step 3: Preprocessing\n",
    "        print(\"\\nStep 3: Preprocessing for training...\")\n",
    "        df_processed = advanced_preprocessing(df)\n",
    "\n",
    "        # Step 4: Splitting\n",
    "        print(\"\\nStep 4: Creating balanced splits...\")\n",
    "        train_df, val_df = create_balanced_splits(df_processed)\n",
    "\n",
    "        # Step 5: Tokenization\n",
    "        print(\"\\nStep 5: Tokenizing data...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"google/mt5-small\")\n",
    "        train_tokenized, val_tokenized = preprocess_data_enhanced(train_df, val_df, tokenizer)\n",
    "\n",
    "        # Step 6: Training\n",
    "        print(\"\\nStep 6: Setting up and training model...\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {device}\")\n",
    "        trainer, tokenizer, model = setup_simple_trainer(train_tokenized, val_tokenized)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save model\n",
    "        model_save_path = \"./enhanced-mt5-financial\"\n",
    "        trainer.save_model(model_save_path)\n",
    "        tokenizer.save_pretrained(model_save_path)\n",
    "        print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "        # Step 7: Testing\n",
    "        print(\"\\nStep 7: Testing model...\")\n",
    "        quick_test_enhanced(model, tokenizer, device)\n",
    "        test_with_your_data(model, tokenizer, val_df, n_samples=5, device=device)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "        print(\"=\"*60)\n",
    "        return trainer, tokenizer, model, train_df, val_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# 12. Compatibility Tests\n",
    "def run_compatibility_tests():\n",
    "    print(\"Running compatibility tests...\")\n",
    "\n",
    "    tests = [\n",
    "        (\"PyTorch\", lambda: __import__(\"torch\")),\n",
    "        (\"Transformers\", lambda: __import__(\"transformers\")),\n",
    "        (\"Datasets\", lambda: __import__(\"datasets\")),\n",
    "        (\"Pandas\", lambda: __import__(\"pandas\")),\n",
    "        (\"NumPy\", lambda: __import__(\"numpy\")),\n",
    "        (\"Matplotlib\", lambda: __import__(\"matplotlib\")),\n",
    "        (\"NLTK\", lambda: __import__(\"nltk\")),\n",
    "        (\"Scikit-learn\", lambda: __import__(\"sklearn\")),\n",
    "    ]\n",
    "\n",
    "    for name, test_func in tests:\n",
    "        try:\n",
    "            test_func()\n",
    "            print(f\"✓ {name} - OK\")\n",
    "        except ImportError:\n",
    "            print(f\"✗ {name} - Missing (install with: pip install {name.lower()})\")\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"✓ CUDA available - {torch.cuda.get_device_name(0)}\" if torch.cuda.is_available() else \"ℹ CUDA not available - will use CPU\")\n",
    "    except:\n",
    "        print(\"✗ Could not check CUDA status\")\n",
    "\n",
    "    print(\"Compatibility test completed.\")\n",
    "\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    run_compatibility_tests()\n",
    "    trainer, tokenizer, model, train_df, val_df = main_pipeline_improved(\"merged.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
